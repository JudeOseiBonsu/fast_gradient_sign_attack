{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ed2c56f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "accd6bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import glob\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "import pathlib\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9479766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#check for device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30519fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b31243f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a86fa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"./train/\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3025254a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([transforms.ToTensor()]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cf4a52d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.ImageFolder(root=train_path, transform=train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0571794e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size =32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5a6a33fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the mean and std to normalize the images\n",
    "\n",
    "def mean_std(loader):\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    total_images = 0\n",
    "    for images, _ in loader:\n",
    "        image_count = images.size(0)\n",
    "        images = images.view(image_count, images.size(1), -1)\n",
    "        mean += images.mean(2).sum(0)\n",
    "        std += images.std(2).sum(0)\n",
    "        total_images += image_count\n",
    "        \n",
    "    mean /= total_images\n",
    "    std  /= total_images\n",
    "    \n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7821d3e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.3230, 0.5492, 0.2603]), tensor([0.2485, 0.0889, 0.1177]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_std(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff474dc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fea89409",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforms\n",
    "transformer = transforms.Compose([\n",
    "    transforms.Resize((150,150)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.3230, 0.5492, 0.2603], [0.2481, 0.0883, 0.1172])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e4ec00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4dddb51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataLoaders for training\n",
    "train_path = \"./train/\"  \n",
    "test_path  = \"./test/\"   \n",
    "\n",
    "train_loader = DataLoader(\n",
    "                torchvision.datasets.ImageFolder(train_path, transform=transformer),\n",
    "                batch_size=16, shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "                torchvision.datasets.ImageFolder(test_path, transform=transformer),\n",
    "                batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3a5906",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18a5546b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['paper', 'rock', 'scissors']\n"
     ]
    }
   ],
   "source": [
    "#lables\n",
    "root = pathlib.Path(train_path)\n",
    "classes = sorted([j.name.split('/')[-1] for j in root.iterdir()])\n",
    "\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9aac968",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c3b4df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5246eef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN Network\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        \n",
    "        \n",
    "        super(ConvNet, self).__init__()\n",
    "    \n",
    "        #output size after convolution filter\n",
    "        #((w-f+2P)/s) + 1\n",
    "    \n",
    "        #Input shape = (16 ,3,150,150)\n",
    "    \n",
    "        self.conv1=nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        #shape = (16, 12, 150, 150)\n",
    "    \n",
    "        self.bn1 = nn.BatchNorm2d(num_features=12) #num_features == out_channels\n",
    "        #shape = (16, 12, 150, 150)\n",
    "    \n",
    "        self.relu1 = nn.ReLU()\n",
    "        #shape = (16, 12, 150, 150)\n",
    "    \n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        # Reduce the image size by factor 2\n",
    "        \n",
    "    \n",
    "        ############################################\n",
    "        #shape = (16, 12, 75, 75)\n",
    "        self.conv2=nn.Conv2d(in_channels=12, out_channels=20, kernel_size=3, stride=1, padding=1)\n",
    "        #shape = (16, 20, 75, 75)\n",
    "    \n",
    "        self.relu2 = nn.ReLU()\n",
    "        #shape = (16, 20, 75, 75)\n",
    "    \n",
    "        #################################################\n",
    "    \n",
    "        self.conv3=nn.Conv2d(in_channels=20, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        #shape = (16, 32, 75, 75)\n",
    "    \n",
    "        self.bn3 = nn.BatchNorm2d(num_features=32) #num_features == out_channels\n",
    "        #shape = (16, 32, 75, 75)\n",
    "    \n",
    "        self.relu3 = nn.ReLU()\n",
    "        #shape = (16, 32, 75, 75)\n",
    "    \n",
    "    \n",
    "        self.fc = nn.Linear(in_features=32*75*75, out_features=3)\n",
    "    \n",
    "    \n",
    "        #Forward Propagation\n",
    "    def forward(self, input):\n",
    "        \n",
    "        output = self.conv1(input)\n",
    "        output = self.bn1(output)\n",
    "        output = self.relu1(output)\n",
    "        \n",
    "        output = self.pool(output)\n",
    "        \n",
    "        output = self.conv2(output)\n",
    "        output = self.relu2(output)\n",
    "        \n",
    "        output = self.conv3(output)\n",
    "        output = self.bn3(output)\n",
    "        output = self.relu3(output)\n",
    "        \n",
    "        \n",
    "        #Above output will be in matrix form,with shape (16,32,75,75)\n",
    "        \n",
    "        \n",
    "        output = output.view(-1, 32*75*75)\n",
    "        \n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7a335b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d96ef23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet(num_classes=3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b0727c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4db0abdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimizer and loss function\n",
    "optimizer = Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cad342",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b533603a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d59f47a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82efdc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the size of training and testing images\n",
    "train_count = len(glob.glob(train_path+\"/**/*.png\"))\n",
    "test_count = len(glob.glob(test_path+\"/**/*.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49744848",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6af76dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2038 150\n"
     ]
    }
   ],
   "source": [
    "print(train_count, test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f628f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44b54093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0loss: tensor(8.3571)Train Accuracy: 0.6957801766437685Test Accuracy: 0.68\n",
      "Epoch: 1loss: tensor(3.1131)Train Accuracy: 0.8635917566241413Test Accuracy: 0.9133333333333333\n",
      "Epoch: 2loss: tensor(1.6465)Train Accuracy: 0.9347399411187438Test Accuracy: 0.88\n",
      "Epoch: 3loss: tensor(1.1830)Train Accuracy: 0.9406280667320903Test Accuracy: 0.8933333333333333\n",
      "Epoch: 4loss: tensor(0.6748)Train Accuracy: 0.965161923454367Test Accuracy: 0.9666666666666667\n",
      "Epoch: 5loss: tensor(0.6956)Train Accuracy: 0.9646712463199215Test Accuracy: 0.9533333333333334\n",
      "Epoch: 6loss: tensor(0.5749)Train Accuracy: 0.9695780176643768Test Accuracy: 0.9533333333333334\n",
      "Epoch: 7loss: tensor(0.8305)Train Accuracy: 0.964180569185476Test Accuracy: 0.9466666666666667\n",
      "Epoch: 8loss: tensor(0.4320)Train Accuracy: 0.9744847890088322Test Accuracy: 0.9866666666666667\n",
      "Epoch: 9loss: tensor(0.2876)Train Accuracy: 0.9779195289499509Test Accuracy: 0.9733333333333334\n",
      "Epoch: 10loss: tensor(0.2432)Train Accuracy: 0.9867517173699706Test Accuracy: 0.9666666666666667\n",
      "Epoch: 11loss: tensor(0.4177)Train Accuracy: 0.9754661432777233Test Accuracy: 0.96\n",
      "Epoch: 12loss: tensor(0.2860)Train Accuracy: 0.9813542688910697Test Accuracy: 0.9866666666666667\n",
      "Epoch: 13loss: tensor(0.6466)Train Accuracy: 0.9710500490677134Test Accuracy: 0.9533333333333334\n",
      "Epoch: 14loss: tensor(0.3847)Train Accuracy: 0.9813542688910697Test Accuracy: 0.94\n",
      "Epoch: 15loss: tensor(0.2012)Train Accuracy: 0.9882237487733072Test Accuracy: 0.9866666666666667\n",
      "Epoch: 16loss: tensor(0.2421)Train Accuracy: 0.9857703631010795Test Accuracy: 0.9666666666666667\n",
      "Epoch: 17loss: tensor(0.2793)Train Accuracy: 0.9838076545632973Test Accuracy: 0.9733333333333334\n",
      "Epoch: 18loss: tensor(0.1291)Train Accuracy: 0.9906771344455348Test Accuracy: 0.9466666666666667\n",
      "Epoch: 19loss: tensor(0.1466)Train Accuracy: 0.9867517173699706Test Accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "#Model training and saving best model\n",
    "\n",
    "best_accuracy = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    #Training of model on train dataset\n",
    "    model.train()\n",
    "    \n",
    "    train_accuracy = 0.0\n",
    "    train_loss= 0.0\n",
    "    \n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            images = Variable(images.cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.cpu().data*images.size(0)\n",
    "        _, prediction = torch.max(outputs.data, 1)\n",
    "        \n",
    "        train_accuracy += int(torch.sum(prediction == labels.data))\n",
    "        \n",
    "    train_accuracy = train_accuracy/train_count\n",
    "    train_loss = train_loss/train_count\n",
    "    \n",
    "    \n",
    "    #Evalutation on testing dataset\n",
    "    model.eval()\n",
    "    \n",
    "    test_accuracy = 0.0\n",
    "    \n",
    "    for i, (images, labels) in enumerate(test_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            images = Variable(images.cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "            \n",
    "        outputs = model(images)\n",
    "        _, prediction = torch.max(outputs.data, 1)\n",
    "        test_accuracy += int(torch.sum(prediction == labels.data))\n",
    "        \n",
    "    test_accuracy = test_accuracy/test_count\n",
    "    \n",
    "    \n",
    "    print(\"Epoch: \"  +  str(epoch) +   \" loss: \" + str(train_loss)          + \"  Train Accuracy: \"+ str(train_accuracy)  +  \"  Test Accuracy: \"+ str(test_accuracy))\n",
    "    \n",
    "    \n",
    "    #save the best model\n",
    "    if test_accuracy>best_accuracy:\n",
    "        torch.save(model.state_dict(), \"best_checkpoint.model\")  \n",
    "        best_accuracy = test_accuracy\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acc7ff4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df25484c",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################  PREDICTION PREDICTION PREDICTION ########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e20cd0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision.models import squeezenet1_1\n",
    "import torch.nn.functional as F\n",
    "from io import open\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e181db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9666ca04",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_path  = \"./pred/\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08af989",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "731298a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (conv1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu1): ReLU()\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(12, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu2): ReLU()\n",
       "  (conv3): Conv2d(20, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu3): ReLU()\n",
       "  (fc): Linear(in_features=180000, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model = torch.load(\"./best_checkpoint.model\")\n",
    "model = ConvNet(num_classes=6)\n",
    "model.load_state_dict(trained_model)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab9788f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "462c3b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prdiction function\n",
    "def prediction(img_path, transformer):\n",
    "    \n",
    "    image = Image.open(img_path)\n",
    "    \n",
    "    image_tensor = transformer(image).float()\n",
    "    \n",
    "    image_tensor = image_tensor.unsqueeze_(0)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        image_tensor.cuda()\n",
    "        \n",
    "    input = Variable(image_tensor)\n",
    "    \n",
    "    output = model(input)\n",
    "    \n",
    "    index = output.data.numpy().argmax()\n",
    "    \n",
    "    pred = classes[index]\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd0f4a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "70e6b8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = glob.glob(pred_path+\"/*.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0aa6e48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ff88c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict = {}\n",
    "\n",
    "for i in images_path:\n",
    "    pred_dict[i[i.rfind(\"/\")+1:]] = prediction(i, transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a037d96c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cd02a15b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pred\\\\paper1.png': 'paper',\n",
       " 'pred\\\\paper2.png': 'paper',\n",
       " 'pred\\\\paper3.png': 'paper',\n",
       " 'pred\\\\paper4.png': 'paper',\n",
       " 'pred\\\\rock1.png': 'rock',\n",
       " 'pred\\\\rock2.png': 'rock',\n",
       " 'pred\\\\rock3.png': 'rock',\n",
       " 'pred\\\\scissors1.png': 'scissors',\n",
       " 'pred\\\\scissors2.png': 'scissors',\n",
       " 'pred\\\\scissors3.png': 'scissors'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e7eb36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847a3d49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cc5660ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################################################\n",
    "############## Adversarial learning  Adversarial Learning  Adversarial learning  Adversarial Learning #####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "16ef396f",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = [0, .05, .1, .15, .2, .25, .3]\n",
    "pretrained_model = \"./best_checkpoint.model\"\n",
    "use_cuda=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67f14f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb49be6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f7286e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (conv1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu1): ReLU()\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(12, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu2): ReLU()\n",
       "  (conv3): Conv2d(20, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu3): ReLU()\n",
       "  (fc): Linear(in_features=180000, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_loader = DataLoader(torchvision.datasets.ImageFolder(root=\"./adv_ex\", transform=transformer),batch_size=1, shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n",
    "\n",
    "\n",
    "#Initialize the network\n",
    "model = ConvNet().to(device)\n",
    "\n",
    "#Load the pretrained model\n",
    "model.load_state_dict(torch.load(pretrained_model, map_location='cpu'))\n",
    "\n",
    "\n",
    "#Set the model in evaluation mode. In this case this is for the Dropout layers\n",
    "model.eval()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4bb66a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ddb8740a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_attack(image, epsilon, data_grad):\n",
    "    # Collect the element-wise sign of the data gradient\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    \n",
    "    # Create the perturbed image by adjusting each pixel of the input image\n",
    "    perturbed_image = image + epsilon*sign_data_grad\n",
    "    \n",
    "    # Adding clipping to maintain [0,1] range\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    \n",
    "    # Return the perturbed image\n",
    "    return perturbed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f371256c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "acf28ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, epsilon ):\n",
    "\n",
    "    # Accuracy counter\n",
    "    correct = 0\n",
    "    adv_examples = []\n",
    "\n",
    "    # Loop over all examples in test set\n",
    "    for data, target in test_loader:\n",
    "\n",
    "        # Send the data and label to the device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Set requires_grad attribute of tensor. Important for Attack\n",
    "        data.requires_grad = True\n",
    "\n",
    "        # Forward pass the data through the model\n",
    "        output = model(data)\n",
    "        init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "\n",
    "        # If the initial prediction is wrong, dont bother attacking, just move on\n",
    "        if init_pred.item() != target.item():\n",
    "            continue\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = F.nll_loss(output, target)\n",
    "\n",
    "        # Zero all existing gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Calculate gradients of model in backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Collect datagrad\n",
    "        data_grad = data.grad.data\n",
    "\n",
    "        # Call FGSM Attack\n",
    "        perturbed_data = fgsm_attack(data, epsilon, data_grad)\n",
    "\n",
    "        # Re-classify the perturbed image\n",
    "        output = model(perturbed_data)\n",
    "\n",
    "        # Check for success\n",
    "        final_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        if final_pred.item() == target.item():\n",
    "            correct += 1\n",
    "            # Special case for saving 0 epsilon examples\n",
    "            if (epsilon == 0) and (len(adv_examples) < 5):\n",
    "                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n",
    "        else:\n",
    "            # Save some adv examples for visualization later\n",
    "            if len(adv_examples) < 5:\n",
    "                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n",
    "\n",
    "    # Calculate final accuracy for this epsilon\n",
    "    final_acc = correct/float(len(test_loader))\n",
    "    print(\"Epsilon: {}\\tTest Accuracy = {}/{} = {}\".format(epsilon, correct, len(test_loader), final_acc))\n",
    "\n",
    "    # Return the accuracy and an adversarial example\n",
    "    return final_acc, adv_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670b3cab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5085c48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon: 0\tTest Accuracy = 7/10 = 0.7\n",
      "Epsilon: 0.05\tTest Accuracy = 7/10 = 0.7\n",
      "Epsilon: 0.1\tTest Accuracy = 7/10 = 0.7\n",
      "Epsilon: 0.15\tTest Accuracy = 6/10 = 0.6\n",
      "Epsilon: 0.2\tTest Accuracy = 5/10 = 0.5\n",
      "Epsilon: 0.25\tTest Accuracy = 4/10 = 0.4\n",
      "Epsilon: 0.3\tTest Accuracy = 2/10 = 0.2\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "examples = []\n",
    "\n",
    "# Run test for each epsilon\n",
    "for eps in epsilons:\n",
    "    acc, ex = test(model=model, device=device, test_loader=pred_loader, epsilon=eps)\n",
    "    accuracies.append(acc)\n",
    "    examples.append(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70f2c67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09f124e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot several examples of adversarial samples at each epsilon\n",
    "cnt = 0\n",
    "plt.figure(figsize=(8,10))\n",
    "for i in range(len(epsilons)):\n",
    "    for j in range(len(examples[i])):\n",
    "        cnt += 1\n",
    "        plt.subplot(len(epsilons),len(examples[0]),cnt)\n",
    "        plt.xticks([], [])\n",
    "        plt.yticks([], [])\n",
    "        if j == 0:\n",
    "            plt.ylabel(\"Eps: {}\".format(epsilons[i]), fontsize=14)\n",
    "        orig,adv,ex = examples[i][j]\n",
    "        plt.title(\"{} -> {}\".format(orig, adv))\n",
    "        plt.imshow(ex[0,:,:], cmap=\"gray\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755e6b87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1be6ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca3623a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fa9389",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5600d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24c5043",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c557ca11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc20ddd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d5c823",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38561f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7922a909",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0ac756",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7994124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69bbf11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c8635a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd4fb89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a83326",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32863bd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057c1c43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81839473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22d0dad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8996b2e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d426c8fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203d4e6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143db7ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fac8361",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdeebc9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ee1f75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684dbc9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c14066",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8535a01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
